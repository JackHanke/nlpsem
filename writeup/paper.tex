% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
% \usepackage[review]{acl}
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% Jack!
% \usepackage{amsmath}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Modern Byte Pair Tokenizers are Zipfian}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Jack Hanke \\
  Northwestern University \\
  \And
  Daniel Plotkin \\
  Northwestern University \\
  \And
  Nicole Birova \\
  Northwestern University}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
A majority of large language models ingest word fragments produced by a data compression algorithm known as byte pair encoding. This algorithm groups high-frequency letter pairings in natural language into individual units called  \textit{tokens}. A natural question is whether the tokens produced by this pairing process deviate significantly from the source language's frequency distribution. Zipf famously showed that many natural language's frequency distribution follows a power law, commonly known as Zipf's Law. We examine two modern tokenizer's adherence to Zipf's law at the token level. We demonstrate that these tokenizers are Zipfian on two corpuses, and speculate as to why this is.
\end{abstract}

\section{Introduction}
\label{section:intro}

George Zipf in his hallmark \textit{The Psycho-Biology of Language} \cite{Zip35} introduced the remarkable trend that the word frequencies in many human languages exhibit the same power law distribution. Specifically, Zipf said the number of times a word appears in a corpus is proportional to the word's frequency rank. Mathematically, Zipf's laws states

\begin{equation}
    \mbox{word frequency} \propto \frac{1}{\mbox{word rank}}.
\end{equation}

A more descriptive model of word frequency that is more commonly referenced in linguistics literature is the Zipf-Mandelbrot distribution

\begin{equation}
    \mbox{word frequency} \propto \frac{1}{(\mbox{word rank} + b)^a}.
    \label{eq:zipfmandelbrot}
\end{equation}

where $a,b$ are fitted parameters. We say a distribution that follows the trend in Equation \ref{eq:zipfmandelbrot} with $a \approx 1$ is \textit{Zipf distributed}, or simply \textit{Zipfian}.

The accuracy of the trend in Equation \ref{eq:zipfmandelbrot} has been examined in $10^8$ English words in \cite{cancho2000}, over 50 languages in \cite{yu2018zipfslaw50languages}, and written-versus-spoken corpuses in \cite{lin2015scalinglawshumanspeech}. Each of these studies demonstrate that Zipf's law is generally exhibitted for common and somewhat-uncommon words, but rare words (words with high rank) appear less frequently than predicted.\footnote{Some of these studies also find that extremely common words appear slighly more commonly than the Zipfian prediction.} This deviation is shown to be statistically significant, and appears as two trendlines in the log-log plot. In English corpuses, this transition is found around the $10^5$\textit{-th} ranked word. The authors further explore the linguistic relevance and universality of these multiple regimes of words. 

Nearly a century after Zipf's discovery, large language models (LLMs) generate text comparable to human communication. However, unlike humans, LLMs digest text using a fixed vocabulary of word fragments called \textit{tokens}. The mapping between natural language and tokens is most commonly computed using the \textit{byte pair encoding (BPE) algorithm} \cite{bpegage} over some large training corpus. Byte pair encoding identifies the most frequent pair of letters, and creates a new token in the "token vocabulary" to replace that pair. Iterating this procedure until some fixed vocabulary size is reached creates the vocabulary of tokens. Finally, any sequence that does not map to one of the derived tokens is labelled with the ``unknown'' token.

When considering these ideas in tandem, a natural question arises: given a corpus that appears Zipfian, how does the tokenization process affect this trend?

\section{Methods}

To explore this question, we examine two modern tokenizers. We choose the tokenizer for the RoBERTa language model \cite{liu2019robertarobustlyoptimizedbert}, as the training data is publicly known. This allows us to conduct frequency analyses on corpuses that are on and off-distribution for the tokenizer. We also choose to compare our results with the tokenizer for the GPT-4 language model \cite{openai2024gpt4technicalreport} as a representative of industry-scale byte pair encoding. 

We choose two corpuses based on RoBERTa's training data. We choose the 4.4GB bookscorpus dataset \cite{zhu2015aligningbooksmoviesstorylike}, which is within RoBERTa's training data, and the 5.6GB MiniPile dataset \cite{kaddour2023minipilechallengedataefficientlanguage}, which is not. Note that it is likely that the GPT-4 tokenizer was trained (in-part) on both of these corpuses.

We compute the word and token frequency for both tokenizers on each corpus, where we define a word as anything separated by a space. We then compute the data's Kolmogorov-Smirnov goodness-of-fit statistic for the fitted Zipf distribution, and compare to that for a fitted exponential and log-normal distribution. 

We do not remove any words or tokens from consideration unless otherwise stated, as to a language model all non-padding tokens are treated the same in training. 

\section{Results}

We find that token vocabularies are remarkably Zipfian, excluding extremely rare tokens. This holds true for both tokenizers and both corpuses studied, including the out-of-distribution text. In the log-log plots in Figure \ref{fig:experiments}, we find a clear linear trend over a large number of words and tokens. We only see deviations from the Zipfian trend for the final few tokens in the RoBERTa tokenizer for both corpuses and the GPT-4 tokenizer for the MiniPile corpus. However, the GPT-4 tokenizer trend on the bookscorpus seems to deviate more dramatically, similar to the multiple regime studies mentioned in Section \ref{section: intro}.

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{../visualizations/bookscorpusfreq.png}
  \includegraphics[width=\columnwidth]{../visualizations/minipilecorpusfreq.png}
  \caption{Log-log plots of word \& token rank vs word \& token frequency}
  \label{fig:experiments}
\end{figure}

We also report the lowest rank words and tokens for each tokenizer and corpus to identify trends across models and corpuses. We report these words and tokens both including and excluding control tokens and punctuation, which are summarized in Table \ref{tbl:token ranks} and Table \ref{tbl:token ranks no control} respectively.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Rank} & \multicolumn{2}{c|}{\textbf{Word}} & \multicolumn{2}{c|}{\textbf{RoBERTa}} & \multicolumn{2}{c|}{\textbf{GPT-4}}  \\
        \hline
        1 & . & the & <s> & ␣ & `` & , \\
        \hline
        2 & , & of & \textbackslash n & \textbackslash n & i & ␣the \\
        \hline
        3 & the & and & <\textbackslash s> & . & he & . \\
        \hline
    \end{tabular}
    \caption{Summary of most common words \& tokens for each corpus, where the left subcolumn is bookscorpus and the right subcolumn MiniPile, including control tokens and punctuation}
    \label{tbl:token ranks}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Rank} & \multicolumn{2}{c|}{\textbf{Word}} & \multicolumn{2}{c|}{\textbf{RoBERTa}} & \multicolumn{2}{c|}{\textbf{GPT-4}}  \\
        \hline
        1 & the & the & the & the & i & the \\
        \hline
        2 & to & of & to & of & he & of \\
        \hline
        3 & i & and & and & and & she & and \\
        \hline
    \end{tabular}
    \caption{Summary of most common words \& tokens for each corpus, where the left subcolumn is bookscorpus and the right subcolumn MiniPile, \textit{not} including control tokens and punctuation. A majority of these words are considered \textit{function words} in linguistics literature.}
    \label{tbl:token ranks no control}
\end{table}

Finally, we report the Kolmogorov-Smirnov test statistics for fitted Zipf-Mandelbrot, Exponential, and log-normal distributions in Table \ref{tbl:kstest}. 

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        \textbf{Distribution} & \multicolumn{2}{c|}{\textbf{Words}} & \multicolumn{2}{c|}{\textbf{RoBERTa}} & \multicolumn{2}{c|}{\textbf{GPT-4}}  \\
        \hline
        Zipf &  &  &  &  &  &  \\
        \hline
        Exponential & 0.87 & 0.81 & 0.58 & 0.48 & 0.72 & 0.56 \\
        \hline
        log-normal & 0.38 & 0.45 & 0.03 & 0.05 & 0.40 & 0.28 \\
        \hline
    \end{tabular}
    \caption{Kolmogorov-Smirnov test statistics for distribution fit, where the left subcolumn is bookscorpus and the right subcolumn MiniPile}
    \label{tbl:kstest}
\end{table}


\section{Analysis}

The BPE algorithm builds a finite vocabulary by directly referencing the frequency of subwords and maps everything not captured to a "catch-all" unknown token. Despite this, these generated vocabularies are still Zipfian up to the rarest few tokens. Moreover, the tokenizers this algorithm produces creates Zipfian distributed tokens outside of the corpus it is trained on. When considering all possible tokens, the most common tokens tend to be punctuation, newline characters, and control tokens. Excluding punctuation and special tokens, we recover the known most common English words. 

\section{Conclusions \& Future Work}

Noticeable theoretical work has been done in service of explaining Zipf's law for language \cite{li1992}. Most significant to the discussion was Belecitch's \textit{On the statistical laws of linguistic distributions}\cite{Belevitch1959}, in which the author shows the first order approximation of the rank ordering of many statistical distributions are all Zipfian. Can this approximation be shown to be invariant under the byte pair encoding transformation?

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

% \appendix

% \section{Example Appendix}
% \label{sec:appendix}

% This is an appendix.

\end{document}